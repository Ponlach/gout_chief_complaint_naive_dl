# -*- coding: utf-8 -*-
"""gout_dl

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15X6_yWq9JsMBc1qg7xT6lKMZIRIjfxfn
"""

import pandas as pd
import numpy as np

df1 = pd.read_csv('GOUT-CC-2020-CORPUS-SYNTHETIC.csv')
df2 = pd.read_csv('GOUT-CC-2019-CORPUS-SYNTHETIC.csv')
df = pd.concat([df1,df2])

df['label']=df['Consensus'].mask(df['Consensus']=="-",df['Predict'])
#df['text'] = df['Chief Complaint'].str.split()
df['text'] = df['Chief Complaint'].str.lower()
data = df[['label', 'text']]
data['label'].value_counts()

from transformers import AutoTokenizer, AutoModelForTokenClassification

tokenizer = AutoTokenizer.from_pretrained("Clinical-AI-Apollo/Medical-NER")
model = AutoModelForTokenClassification.from_pretrained("Clinical-AI-Apollo/Medical-NER")

documents = list(data.text)
categories = list(data.label)
#tokenized_inputs = tokenizer(documents, return_tensors="pt", padding=True, truncation=True)

from transformers import pipeline
pipe = pipeline("token-classification", model="Clinical-AI-Apollo/Medical-NER", aggregation_strategy='simple')
ner_results = pipe(documents)

import json
X = []
for item in ner_results:
  x_i = ""
  for i in item:
    x_i+=str(i['word'])+" "
  X.append(x_i)

Y2 = [0 if x == 'N' else 1 if x == 'Y' else 2 for x in categories]
Y = [1 if x == 'Y' else 0 for x in categories]

from collections import Counter
Counter(Y)

import random
index_y = (random.sample(np.where(np.array(Y)==1)[0].tolist(), k = 100))

index_x = range(1,8437)
index_x = [e for e in index_x if e not in index_y]
index_x = (random.sample(index_x, k = 100))

x_train = [X[i] for i in index_y]+[X[i] for i in index_x]
y_train = [Y[i] for i in index_y]+[Y[i] for i in index_x]

from tensorflow.keras import layers, models, metrics
import torch.nn.functional as F

model = models.Sequential()
model.add(layers.Embedding(300000, 128, input_length=84))
model.add(layers.Conv1D(64, 3, activation='relu'))
model.add(layers.MaxPooling1D())
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(2))

model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=[metrics.BinaryAccuracy()])

history = model.fit(x_train, y_train, epochs=10)

pred = model.predict(X)

Y_pred_classes = np.argmax(pred,axis = 1)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_recall_fscore_support as prfs

def showConfusionMatrix(y_true,y_pred):
    regex1data  = {'y_Actual': y_true,'y_Predicted': y_pred}
    df = pd.DataFrame(regex1data, columns=['y_Actual','y_Predicted'])
    confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'],)
    ax = plt.axes()
    ax = sns.heatmap(confusion_matrix, annot=True,fmt="d",ax=ax)
    plt.show()

showConfusionMatrix(Y,Y_pred_classes)
p_micro, r_micro, f1_micro, _ = prfs(y_pred=Y_pred_classes, y_true=Y, average="micro")
print("Micro Evaluation: Precision: %.4f; Recall: %.4f; F1-Score: %.4f" % (p_micro, r_micro, f1_micro))
p_macro, r_macro, f1_macro, _ = prfs(y_pred=Y_pred_classes, y_true=Y, average="macro")
print("Macro Evaluation: Precision: %.4f; Recall: %.4f; F1-Score: %.4f" % (p_macro, r_macro, f1_macro))

k = 1
for i in list(tokenized_inputs['input_ids']):
  m = max(i)
  if m > k:
    k = m
    print(k)

model.summary()
